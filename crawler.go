package main

import (
	"crypto/tls"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"runtime"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/fatih/color"
)

type Config struct {
	TargetURL    string
	MaxDepth     int
	OnlyInternal bool
	OnlyExternal bool
	OutputPath   string
	Verbose      bool
	ShowTree     bool
}

// Crawler représente un crawler web optimisé.
type Crawler struct {
	Config     Config
	Client     *http.Client
	FastClient *http.Client // Client rapide pour HEAD requests
	Visited    sync.Map     // Map concurrente pour éviter les locks
	Results    []string
	resultsMu  sync.Mutex
	wg         sync.WaitGroup
	validCache sync.Map // Cache de validation des liens
	semaphore  chan struct{}
}

func New(cfg Config) *Crawler {
	workers := runtime.NumCPU() * 4
	if workers < 16 {
		workers = 16
	}

	// Transport optimisé avec pool de connexions
	transport := &http.Transport{
		TLSClientConfig:     &tls.Config{InsecureSkipVerify: true},
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 10,
		MaxConnsPerHost:     20,
		IdleConnTimeout:     30 * time.Second,
		DisableKeepAlives:   false,
	}

	return &Crawler{
		Config: cfg,
		Client: &http.Client{
			Timeout:   8 * time.Second,
			Transport: transport,
		},
		FastClient: &http.Client{
			Timeout:   3 * time.Second, // Timeout court pour HEAD
			Transport: transport,
		},
		semaphore: make(chan struct{}, workers),
	}
}

// Start lance le crawler.
func (c *Crawler) Start() error {
	parsed, err := url.Parse(c.Config.TargetURL)
	if err != nil {
		return err
	}
	norm := parsed.String()
	c.Visited.Store(norm, true)

	if err := c.crawl(norm, 0); err != nil {
		return err
	}
	c.wg.Wait()
	return nil
}

// crawl explore récursivement les liens.
func (c *Crawler) crawl(rawURL string, depth int) error {
	if depth >= c.Config.MaxDepth {
		return nil
	}
	parsed, err := url.Parse(rawURL)
	if err != nil {
		return err
	}

	resp, err := c.Client.Get(rawURL)
	if err != nil {
		if c.Config.Verbose {
			fmt.Printf("[%s] %s: %v\n", color.RedString("ERR"), rawURL, err)
		}
		return nil
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return err
	}

	links := Extract(string(body))
	validLinks := c.validateLinksParallel(links, parsed)

	for _, linkInfo := range validLinks {
		abs := linkInfo.url
		isExternal := linkInfo.isExternal

		if _, loaded := c.Visited.LoadOrStore(abs, true); loaded {
			continue
		}

		if isExternal {
			if !c.Config.OnlyInternal {
				fmt.Printf("[%s] %s\n", color.CyanString("EXT"), abs)
				c.addResult(abs)
			}
		} else {
			if !c.Config.OnlyExternal {
				fmt.Printf("[%s] %s\n", color.GreenString("INT"), abs)
				c.addResult(abs)
			}

			c.wg.Add(1)
			go func(url string, d int) {
				defer c.wg.Done()
				c.semaphore <- struct{}{}
				defer func() { <-c.semaphore }()
				c.crawl(url, d+1)
			}(abs, depth)
		}
	}
	return nil
}

type linkInfo struct {
	url        string
	isExternal bool
}

// validateLinksParallel valide plusieurs liens en parallèle.
func (c *Crawler) validateLinksParallel(links []string, baseURL *url.URL) []linkInfo {
	results := make(chan linkInfo, len(links))
	var wg sync.WaitGroup

	for _, link := range links {
		wg.Add(1)
		go func(l string) {
			defer wg.Done()
			c.semaphore <- struct{}{}
			defer func() { <-c.semaphore }()

			res, err := baseURL.Parse(l)
			if err != nil {
				return
			}
			abs := res.String()
			isExternal := res.Host != baseURL.Host

			// Optimization: Filtre avant validation réseau
			if c.Config.OnlyInternal && isExternal {
				return
			}
			if c.validateLink(abs) {
				results <- linkInfo{
					url:        abs,
					isExternal: isExternal,
				}
			}
		}(link)
	}

	go func() {
		wg.Wait()
		close(results)
	}()

	var validated []linkInfo
	for li := range results {
		validated = append(validated, li)
	}
	return validated
}

// validateLink vérifie qu'un lien est valide (avec cache).
func (c *Crawler) validateLink(u string) bool {
	// Vérifier le cache
	if cached, ok := c.validCache.Load(u); ok {
		return cached.(bool)
	}

	req, err := http.NewRequest("HEAD", u, nil)
	if err != nil {
		c.validCache.Store(u, false)
		return false
	}

	resp, err := c.FastClient.Do(req)
	if err != nil {
		if c.Config.Verbose {
			fmt.Printf("[%s] %s: %v\n", color.RedString("ERR"), u, err)
		}
		c.validCache.Store(u, false)
		return false
	}
	defer resp.Body.Close()

	valid := resp.StatusCode >= 200 && resp.StatusCode < 400
	c.validCache.Store(u, valid)
	return valid
}

func (c *Crawler) addResult(url string) {
	c.resultsMu.Lock()
	c.Results = append(c.Results, url)
	c.resultsMu.Unlock()
}

// SaveJSON sauvegarde les résultats en JSON.
func (c *Crawler) SaveJSON() error {
	if c.Config.OutputPath == "" {
		return nil
	}
	type Export struct {
		Target  string    `json:"target"`
		Results []string  `json:"results"`
		Tree    *treeNode `json:"tree,omitempty"`
		Count   int       `json:"count"`
	}

	var tree *treeNode
	if c.Config.ShowTree {
		tree = c.buildTree()
	}

	data := Export{
		Target:  c.Config.TargetURL,
		Results: c.Results,
		Tree:    tree,
		Count:   len(c.Results),
	}
	file, err := os.Create(c.Config.OutputPath)
	if err != nil {
		return err
	}
	defer file.Close()
	encoder := json.NewEncoder(file)
	encoder.SetIndent("", "  ")
	return encoder.Encode(data)
}

type treeNode struct {
	Name     string               `json:"name"`
	Children map[string]*treeNode `json:"children,omitempty"`
}

func newTreeNode(name string) *treeNode {
	return &treeNode{
		Name:     name,
		Children: make(map[string]*treeNode),
	}
}

func (c *Crawler) PrintTree() {
	if !c.Config.ShowTree {
		return
	}
	fmt.Printf("\n%s\n%s\n", color.MagentaString("=== Site Tree ==="), c.Config.TargetURL)

	root := c.buildTree()
	c.printRecursive(root, "")
}

func (c *Crawler) printRecursive(node *treeNode, prefix string) {
	keys := make([]string, 0, len(node.Children))
	for k := range node.Children {
		keys = append(keys, k)
	}
	sort.Strings(keys)

	for i, name := range keys {
		isLast := i == len(keys)-1
		connector := "├── "
		if isLast {
			connector = "└── "
		}
		fmt.Printf("%s%s%s\n", prefix, connector, name)

		newPrefix := prefix + "│   "
		if isLast {
			newPrefix = prefix + "    "
		}
		c.printRecursive(node.Children[name], newPrefix)
	}
}

func (c *Crawler) buildTree() *treeNode {
	rootURL, _ := url.Parse(c.Config.TargetURL)
	root := newTreeNode("/")

	urls := append([]string{c.Config.TargetURL}, c.Results...)
	for _, uStr := range urls {
		u, err := url.Parse(uStr)
		if err != nil || u.Host != rootURL.Host {
			continue
		}

		path := u.Path
		if path == "" {
			path = "/"
		}

		suffix := ""
		if u.RawQuery != "" {
			suffix = "?" + u.RawQuery
		}

		parts := strings.Split(path, "/")
		current := root

		for i, part := range parts {
			if part == "" {
				continue
			}
			name := part
			if i == len(parts)-1 {
				name += suffix
			}
			if _, exists := current.Children[name]; !exists {
				current.Children[name] = newTreeNode(name)
			}
			current = current.Children[name]
		}

		if path == "/" && suffix != "" {
			name := "?" + u.RawQuery
			if _, exists := root.Children[name]; !exists {
				root.Children[name] = newTreeNode(name)
			}
		}
	}
	return root
}
